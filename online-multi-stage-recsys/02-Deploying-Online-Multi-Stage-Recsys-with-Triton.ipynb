{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_02-deploying-multi-stage-recsys-with-merlin-systems/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Deploying Online Multi-Stage RecSys with Triton Inference Server\n",
    "\n",
    "At this point, we expect that you have already executed the first notebook `01-Building-Online-Multi-Stage-Recsys-Components.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![img](./img/OnlineMultiStageRecsys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "We will serve the multi-stage recommender on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently. Below, we will go through these steps and demonstrate their usage in serving a multi-stage system on Triton.\n",
    "\n",
    "\n",
    "This notebook is created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a998adb-4e51-46c3-ac79-31242ec28140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf==3.20 in /usr/local/lib/python3.8/dist-packages (3.20.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install protobuf==3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e18d7-b3e4-481c-b69e-23193b212c56",
   "metadata": {},
   "source": [
    "At this step, we assume you already installed feast and faiss-gpu (or -cpu) libraries when running the first notebook `01-Building-Recommender-Systems-with-Merlin.ipynb`. \n",
    "\n",
    "In case you need to install them for running this example on GPU, execute the following script in a cell.\n",
    "```\n",
    "%pip install \"feast<0.20\" faiss-gpu\n",
    "```\n",
    "or the following script in a cell for CPU.\n",
    "```\n",
    "%pip install tensorflow-cpu \"feast<0.20\" faiss-cpu\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import feast\n",
    "import faiss\n",
    "\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request\n",
    "\n",
    "import tensorflow as tf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b38d23b-dcdd-4e6c-998d-5ed9a5d1c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path for data\n",
    "BASE_DIR = os.environ['PWD']\n",
    "FEATURE_STORE_DIR = os.path.join(BASE_DIR, \"feature_repo/\")\n",
    "MODEL_DIR = os.path.join(BASE_DIR, \"models/\")\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "DLRM_DIR = os.path.join(DATA_DIR, \"dlrm\")\n",
    "QUERY_TOWER_DIR = os.path.join(DATA_DIR, \"query_tower\")\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "OUTPUT_RETRIEVAL_DATA_DIR = os.path.join(OUTPUT_DATA_DIR, \"retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5af3c4e-472c-48c0-a185-85ba6f1ada06",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store = feast.FeatureStore(FEATURE_STORE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61271d6e-6cd5-4d22-9e1d-21bc0745d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = feature_store.get_feature_view(\"item_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388448f3-6d26-4f32-9001-363d81144f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item_id'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view.entities[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098dc29a-d0d6-4764-9af6-8eb25ec7d7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[item_category-Int32, item_shop-Int32, item_brand-Int32, item_id_raw-Int32]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768637c-0a4d-404b-8b58-7182fef0ab0e",
   "metadata": {},
   "source": [
    "## Define Triton Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking model, retrieval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_model_path = os.path.join(MODEL_DIR, \"candidate-retrieval/1/model.savedmodel/\")\n",
    "\n",
    "if not os.path.isdir(retrieval_model_path):\n",
    "    shutil.copytree(QUERY_TOWER_DIR, retrieval_model_path)\n",
    "\n",
    "    \n",
    "ranking_model_path = os.path.join(MODEL_DIR, \"ranking/1/model.savedmodel/\")\n",
    "\n",
    "if not os.path.isdir(ranking_model_path):\n",
    "    shutil.copytree(DLRM_DIR, ranking_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d4827fc-673b-40c5-b6de-c335ebbf6b35",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<PrimitiveFeastType.INT32: 3>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msystems\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryFeast \n\u001b[0;32m----> 3\u001b[0m user_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>>\u001b[39m \u001b[43mQueryFeast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_feature_view\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mview\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id_raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/systems/dag/ops/feast.py:90\u001b[0m, in \u001b[0;36mQueryFeast.from_feature_view\u001b[0;34m(cls, store, view, column, output_prefix, include_id, suffix_int)\u001b[0m\n\u001b[1;32m     88\u001b[0m output_schema \u001b[38;5;241m=\u001b[39m Schema([])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_view\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m---> 90\u001b[0m     feature_dtype, is_list, is_ragged \u001b[38;5;241m=\u001b[39m \u001b[43mfeast_2_numpy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list:\n\u001b[1;32m     93\u001b[0m         mh_features\u001b[38;5;241m.\u001b[39mappend(feature\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[0;31mKeyError\u001b[0m: <PrimitiveFeastType.INT32: 3>"
     ]
    }
   ],
   "source": [
    "from merlin.systems.dag.ops.feast import QueryFeast \n",
    "\n",
    "user_features = [\"user_id_raw\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"user_features\",\n",
    "    column=\"user_id_raw\",\n",
    "    include_id=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e25be7-3ff0-49c2-a3fc-03ec4d615e77",
   "metadata": {},
   "source": [
    "Retrieve top-K candidate items using `retrieval model` that are relevant for a given user. We use `PredictTensorflow()` operator that takes a tensorflow model and packages it correctly for TIS to run with the tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21139caa-3a51-42e6-b006-21a92c95f1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 20:04:42.149365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-17 20:04:42.151577: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-17 20:04:42.153074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.dlpack.dlpack.from_dlpack(dlcapsule)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevent TF to claim all GPU memory\n",
    "from merlin.models.loader.tf_utils import configure_tensorflow\n",
    "\n",
    "configure_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c2d9b1-51dc-4549-977d-d7941ee6486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-14 15:28:46.303447: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-14 15:28:47.443330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16249 MB memory:  -> device: 0, name: Quadro GV100, pci bus id: 0000:2d:00.0, compute capability: 7.0\n",
      "09/14/2022 03:28:49 PM WARNING:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "topk_retrieval = int(\n",
    "    os.environ.get(\"topk_retrieval\", \"100\")\n",
    ")\n",
    "retrieval = (\n",
    "    PredictTensorflow(QUERY_TOWER_DIR)\n",
    "    >> QueryFaiss(faiss_index_path, topk=topk_retrieval)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4429c-1fe1-4304-bcdf-badebe3b5485",
   "metadata": {},
   "source": [
    "Fetch item features for the candidate items that are retrieved from the retrieval step above from the feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b270f663-0ae1-4356-acd4-5f8c986abf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = retrieval[\"candidate_ids\"] >> QueryFeast.from_feature_view(\n",
    "    store=feature_store,\n",
    "    view=\"item_features\",\n",
    "    column=\"candidate_ids\",\n",
    "    output_prefix=\"item\",\n",
    "    include_id=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a4d09-db05-4666-b520-75dbbbc7ab17",
   "metadata": {},
   "source": [
    "Merge the user features and items features to create the all set of combined features that were used in model training using `UnrollFeatures` operator which takes a target column and joins the \"unroll\" columns to the target. This helps when broadcasting a series of user features to a set of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb0ef434-03a5-4a36-afb9-e19a43243c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_to_unroll = [\n",
    "    \"user_id\",\n",
    "    \"user_shops\",\n",
    "    \"user_profile\",\n",
    "    \"user_group\",\n",
    "    \"user_gender\",\n",
    "    \"user_age\",\n",
    "    \"user_consumption_2\",\n",
    "    \"user_is_occupied\",\n",
    "    \"user_geography\",\n",
    "    \"user_intentions\",\n",
    "    \"user_brands\",\n",
    "    \"user_categories\",\n",
    "]\n",
    "\n",
    "combined_features = item_features >> UnrollFeatures(\n",
    "    \"item_id\", user_features[user_features_to_unroll]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0ce66-6b6c-43be-885e-a5435c3bbd9e",
   "metadata": {},
   "source": [
    "Rank the combined features using the trained ranking model, which is a DLRM model for this example. We feed the path of the ranking model to `PredictTensorflow()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce31723e-af4d-4827-bb60-3a9fafcd9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = combined_features >> PredictTensorflow(ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f86fa47-de61-4007-ab55-9076e12ce963",
   "metadata": {},
   "source": [
    "For the ordering we use `SoftmaxSampling()` operator. This operator sorts all inputs in descending order given the input ids and prediction introducing some randomization into the ordering by sampling items from the softmax of the predicted relevance scores, and finally returns top-k ordered items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f65598b-e3e7-4238-a73e-19d00c3deb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=10\n",
    "ordering = combined_features[\"item_id_raw\"] >> SoftmaxSampling(\n",
    "    relevance_col=ranking[\"click/binary_classification_task\"], topk=top_k, temperature=20.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2e389-d884-44a1-8e32-4916a0eb43cf",
   "metadata": {},
   "source": [
    "### Export Graph as Ensemble\n",
    "The last step is to create the ensemble artifacts that TIS can consume. To make these artifacts import the Ensemble class. This class  represents an entire ensemble consisting of multiple models that run sequentially in TIS initiated by an inference request. It is responsible with interpreting the graph and exporting the correct files for TIS.\n",
    "\n",
    "When we create an Ensemble object we feed the graph and a schema representing the starting input of the graph.  After we create the ensemble object, we export the graph, supplying an export path for the `ensemble.export()` function. This returns an ensemble config which represents the entire inference pipeline and a list of node-specific configs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc2e4f-5e58-4ad4-8ae5-d79ad286978f",
   "metadata": {},
   "source": [
    "Create the folder to export the models and config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b28c452f-543c-45a4-9995-130ca6919669",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL):\n",
    "    os.makedirs(os.path.join(BASE_DIR, 'poc_ensemble'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061bd82-e553-4823-8d14-3ae88a458c14",
   "metadata": {},
   "source": [
    "Create a request schema that we are going to use when sending a request to Triton Inference Server (TIS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c8b7b94-5559-4587-a272-4d9de2d53dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_schema = Schema(\n",
    "    [\n",
    "        ColumnSchema(\"user_id_raw\", dtype=np.int32),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c64d686-aed5-42f8-b517-482b4237c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ordered_ids']\n"
     ]
    }
   ],
   "source": [
    "# define the path where all the models and config files exported to\n",
    "export_path = os.path.join(BASE_DIR, 'poc_ensemble')\n",
    "\n",
    "ensemble = Ensemble(ordering, request_schema)\n",
    "ens_config, node_configs = ensemble.export(export_path)\n",
    "\n",
    "# return the output column name\n",
    "outputs = ensemble.graph.output_schema.column_names\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276eedd8-5dc0-4ad0-8725-c8da60fea693",
   "metadata": {},
   "source": [
    "Let's check our export_path structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89182219-40a6-458c-af0e-7a8e83f364aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poc_ensemble/\n",
      "├─0_queryfeast/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─1_predicttensorflow/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─assets/\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2_queryfaiss/\n",
      "│ ├─1/\n",
      "│ │ ├─index.faiss/\n",
      "│ │ │ └─index.faiss\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─3_queryfeast/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─4_unrollfeatures/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─5_predicttensorflow/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─assets/\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─6_softmaxsampling/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "└─ensemble_model/\n",
      "  ├─1/\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "sd.seedir(export_path, style='lines', itemlimit=10, depthlimit=5, exclude_folders=['.ipynb_checkpoints', '__pycache__'], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7962cc-f26d-4a4a-b5a3-d214e0f37456",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Starting Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07c620-7d6c-4275-87fe-e5b94335bdb9",
   "metadata": {},
   "source": [
    "It is time to deploy all the models as an ensemble model to Triton Inference Serve [TIS](https://github.com/triton-inference-server). After we export the ensemble, we are ready to start the TIS. You can start triton server by using the following command on your terminal:\n",
    "\n",
    "```\n",
    "tritonserver --model-repository=/ensemble_export_path/ --backend-config=tensorflow,version=2\n",
    "```\n",
    "\n",
    "For the `--model-repository` argument, specify the same path as the `export_path` that you specified previously in the `ensemble.export` method. This command will launch the server and load all the models to the server. Once all the models are loaded successfully, you should see `READY` status printed out in the terminal for each loaded model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0794b1-b9e0-4508-bf6e-cc823ac5c693",
   "metadata": {},
   "source": [
    "Once our models are successfully loaded to the TIS, we can now easily send a request to TIS and get a response for our query with `send_triton_request` utility function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9efbde-4dac-42f1-9ace-096f75bac2b5",
   "metadata": {},
   "source": [
    "Let's send a request to TIS for a given `user_id_raw` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d08a8975-9c32-467b-99ec-df66319f854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id_raw\n",
      "0            7\n"
     ]
    }
   ],
   "source": [
    "# read in data for request\n",
    "from merlin.core.dispatch import make_df\n",
    "\n",
    "# create a request to be sent to TIS\n",
    "request = make_df({\"user_id_raw\": [7]})\n",
    "request[\"user_id_raw\"] = request[\"user_id_raw\"].astype(np.int32)\n",
    "print(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9e27f-6658-4302-b142-08b05215e48f",
   "metadata": {},
   "source": [
    "Let's return raw item ids from TIS as top-k recommended items per given request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ec62f2-5935-45c6-8058-e1cdade6f80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ordered_ids': array([[117],\n",
       "        [415],\n",
       "        [228],\n",
       "        [985],\n",
       "        [ 76],\n",
       "        [410],\n",
       "        [193],\n",
       "        [120],\n",
       "        [ 87],\n",
       "        [139]], dtype=int32)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = send_triton_request(request_schema, request, outputs)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "That's it! You finished deploying a multi-stage Recommender Systems on Triton Inference Server using Merlin framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow-inference:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
