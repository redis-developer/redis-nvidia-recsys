{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_02-deploying-multi-stage-recsys-with-merlin-systems/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Deploying Online Multi-Stage RecSys with Triton Inference Server\n",
    "\n",
    "At this point, we expect that you have already executed the first notebook `01-Building-Online-Multi-Stage-Recsys-Components.ipynb` and exported all the required files and models. Note that even if you didn't run the first notebook, you can still obtain the datasets and pre-trained models by running\n",
    "\n",
    "```bash\n",
    "cd Redis-Recsys\n",
    "aws s3 cp s3://redisventures/merlin/merlin-recsys-data.zip ./data\n",
    "```\n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/RedisVentures/Redis-Recsys/master/assets/OnlineMultiStageRecsys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "We will serve the multi-stage recommender on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import feast\n",
    "import shutil\n",
    "\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b38d23b-dcdd-4e6c-998d-5ed9a5d1c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path for data\n",
    "BASE_DIR = \"/workdir\"\n",
    "FEATURE_STORE_DIR = os.path.join(BASE_DIR, \"feature_repo/\")\n",
    "TRITON_MODEL_REPO = os.path.join(BASE_DIR, \"models/\")\n",
    "\n",
    "DATA_DIR = \"/model-data/\"\n",
    "DLRM_DIR = os.path.join(DATA_DIR, \"dlrm\")\n",
    "QUERY_TOWER_DIR = os.path.join(DATA_DIR, \"query_tower\")\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "OUTPUT_RETRIEVAL_DATA_DIR = os.path.join(OUTPUT_DATA_DIR, \"retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768637c-0a4d-404b-8b58-7182fef0ab0e",
   "metadata": {},
   "source": [
    "## Define Triton Ensemble\n",
    "In order to run our Recsys in Triton, we need to assemble the pieces that will run together as an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd44a-5a7f-48d9-a363-2094bb24cf90",
   "metadata": {},
   "source": [
    "### Setup Triton Model Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking and retrieval model in the Triton Model Repo. We need to move/copy our trained models from the `DATA_DIR` to the `TRITON_MODEL_REPO` so it can be consumed by Triton on startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_model_path = os.path.join(TRITON_MODEL_REPO, \"1-user-embeddings/1/model.savedmodel/\")\n",
    "ranking_model_path = os.path.join(TRITON_MODEL_REPO, \"5-ranking/1/model.savedmodel/\")\n",
    "\n",
    "# Copy over pretrined Query Tower Model to our Triton Model Repository\n",
    "if not os.path.isdir(retrieval_model_path):\n",
    "    shutil.copytree(QUERY_TOWER_DIR, retrieval_model_path)\n",
    "\n",
    "# Copy over pretrined DLRMfor ranking to our Triton Model Repository\n",
    "if not os.path.isdir(ranking_model_path):\n",
    "    shutil.copytree(DLRM_DIR, ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f38180-3280-4ba6-9f8e-2586b93b9be8",
   "metadata": {},
   "source": [
    "### Explore Triton Model Repo\n",
    "Below we will take a look at the multi-stage ensemble that functions as a DAG of operations within Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99860891-c930-4d34-949a-46187ea77f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/\n",
      "├─0-query-user-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─1-user-embeddings/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2-redis-vss-candidates/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─3-query-item-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─4-unroll-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─5-ranking/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ └─input_schema.json\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─6-softmax-sampling/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "└─ensemble-model/\n",
      "  ├─1/\n",
      "  │ └─.gitkeep\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "sd.seedir(\n",
    "    TRITON_MODEL_REPO,\n",
    "    style='lines',\n",
    "    itemlimit=10,\n",
    "    depthlimit=5,\n",
    "    exclude_folders=['.ipynb_checkpoints', '__pycache__'],\n",
    "    sort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86781b3-3d1d-4113-bb52-2a86a16037d1",
   "metadata": {},
   "source": [
    "The subfolders (**starting with 0-6**) in the model repo above represent distinct stages in the RecSys ensemble.\n",
    "\n",
    "- `0-query-user-features/` - fetch user features from Redis.\n",
    "- `1-user-embeddings/` - generate user embeddings from the Query Tower (Tensorflow) model.\n",
    "- `2-redis-vss-candidates/` - perform VSS to find KNN items using RediSearch.\n",
    "- `3-query-item-features/` - fetch item features from Redis.\n",
    "- `4-unroll-features/` - combine and unroll user and item features.\n",
    "- `5-ranking/` - rank the top User/Item pairs with the DLRM (Tensorflow) model.\n",
    "- `6-softmax-sampling/` - sort all inputs in descending order, introduce some randomization via softmax sampling, and return top-k ordered items.\n",
    "- `ensemble-model/`\n",
    "\n",
    "The `ensemble-model` contains the orchestration of all of the individual steps. To learn more about general Triton model repo structure [check this out](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7962cc-f26d-4a4a-b5a3-d214e0f37456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Starting Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07c620-7d6c-4275-87fe-e5b94335bdb9",
   "metadata": {},
   "source": [
    "Now we can deploy all the models as an ensemble model to Triton Inference Serve [TIS](https://github.com/triton-inference-server). After we export the ensemble, we are ready to start the TIS. You can start triton server by using the following command in a Jupyter terminal:\n",
    "\n",
    "```bash\n",
    "tritonserver --model-repository=/workdir/models --backend-config=tensorflow,version=2\n",
    "```\n",
    "\n",
    "*For the `--model-repository` argument, specify the path to the Triton Model Repo stored in the var `TRITON_MODEL_REPO`.* This command will launch the server and load all the models to the server. Once all the models are loaded successfully, you should see `READY` status printed out in the terminal for each loaded model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0794b1-b9e0-4508-bf6e-cc823ac5c693",
   "metadata": {},
   "source": [
    "Once our models are successfully loaded to the TIS, we can now easily send a request to TIS and get a response for our query with our simple Python `client`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9efbde-4dac-42f1-9ace-096f75bac2b5",
   "metadata": {},
   "source": [
    "Let's send a request to TIS for a given `user_id_raw` value. If you make multiple requests in a row for same user, you should see slightly different results based on the randomness introduced via softmax sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d08a8975-9c32-467b-99ec-df66319f854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding recommendations for User 23\n",
      "Recommended Product Ids in 0.7311577796936035 seconds\n",
      "[[ 39]\n",
      " [260]\n",
      " [107]\n",
      " [ 91]\n",
      " [461]\n",
      " [ 47]\n",
      " [ 97]\n",
      " [105]\n",
      " [153]\n",
      " [ 58]\n",
      " [487]\n",
      " [296]\n",
      " [217]\n",
      " [204]\n",
      " [113]\n",
      " [180]]\n"
     ]
    }
   ],
   "source": [
    "!python client.py --user 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87bc9f6c-4f24-47d5-921a-6dc1fa903eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 2\n",
      "  Client: \n",
      "    Request count: 769\n",
      "    Throughput: 42.7175 infer/sec\n",
      "    Avg latency: 46744 usec (standard deviation 20083 usec)\n",
      "    p50 latency: 36466 usec\n",
      "    p90 latency: 85368 usec\n",
      "    p95 latency: 86602 usec\n",
      "    p99 latency: 88286 usec\n",
      "    Avg HTTP time: 46738 usec (send/recv 51 usec + response wait 46687 usec)\n",
      "  Server: \n",
      "    Inference count: 1541\n",
      "    Execution count: 1541\n",
      "    Successful request count: 1541\n",
      "    Avg request latency: 46971 usec (overhead 16 usec + queue 13060 usec + compute 33895 usec)\n",
      "\n",
      "  Composing models: \n",
      "  0-query-user-features, version: \n",
      "      Inference count: 1543\n",
      "      Execution count: 1543\n",
      "      Successful request count: 1543\n",
      "      Avg request latency: 6025 usec (overhead 29 usec + queue 30 usec + compute input 22 usec + compute infer 5837 usec + compute output 106 usec)\n",
      "\n",
      "  1-user-embeddings, version: \n",
      "      Inference count: 1543\n",
      "      Execution count: 1543\n",
      "      Successful request count: 1543\n",
      "      Avg request latency: 1348 usec (overhead 16 usec + queue 56 usec + compute input 58 usec + compute infer 1209 usec + compute output 8 usec)\n",
      "\n",
      "  2-redis-vss-candidates, version: \n",
      "      Inference count: 1543\n",
      "      Execution count: 1543\n",
      "      Successful request count: 1543\n",
      "      Avg request latency: 814 usec (overhead 11 usec + queue 26 usec + compute input 13 usec + compute infer 735 usec + compute output 28 usec)\n",
      "\n",
      "  3-query-item-features, version: \n",
      "      Inference count: 1542\n",
      "      Execution count: 1542\n",
      "      Successful request count: 1542\n",
      "      Avg request latency: 36174 usec (overhead 15 usec + queue 12829 usec + compute input 19 usec + compute infer 23223 usec + compute output 87 usec)\n",
      "\n",
      "  4-unroll-features, version: \n",
      "      Inference count: 1542\n",
      "      Execution count: 1542\n",
      "      Successful request count: 1542\n",
      "      Avg request latency: 825 usec (overhead 27 usec + queue 45 usec + compute input 48 usec + compute infer 599 usec + compute output 105 usec)\n",
      "\n",
      "  5-ranking, version: \n",
      "      Inference count: 1541\n",
      "      Execution count: 1541\n",
      "      Successful request count: 1541\n",
      "      Avg request latency: 1517 usec (overhead 20 usec + queue 50 usec + compute input 67 usec + compute infer 1370 usec + compute output 9 usec)\n",
      "\n",
      "  6-softmax-sampling, version: \n",
      "      Inference count: 1541\n",
      "      Execution count: 1541\n",
      "      Successful request count: 1541\n",
      "      Avg request latency: 383 usec (overhead 13 usec + queue 24 usec + compute input 13 usec + compute infer 270 usec + compute output 62 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 2, throughput: 42.7175 infer/sec, latency 46744 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m ensemble-model -u localhost:8000 --input-data=sample.json --shape=user_id_raw:1,1 -t 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db696892-936b-4021-8549-8b89b0196f1f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "That's it! You finished deploying an online multi-stage Recommender Systems on Triton Inference Server with Redis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow-inference:latest"
   ]
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
