{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_02-deploying-multi-stage-recsys-with-merlin-systems/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Deploying Online Multi-Stage RecSys with Triton Inference Server\n",
    "\n",
    "At this point, we expect that you have already executed the first notebook `01-Building-Online-Multi-Stage-Recsys-Components.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![img](./img/OnlineMultiStageRecsys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "We will serve the multi-stage recommender on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import feast\n",
    "import shutil\n",
    "\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b38d23b-dcdd-4e6c-998d-5ed9a5d1c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path for data\n",
    "BASE_DIR = os.environ['PWD']\n",
    "FEATURE_STORE_DIR = os.path.join(BASE_DIR, \"feature_repo/\")\n",
    "TRITON_MODEL_REPO = os.path.join(BASE_DIR, \"models/\")\n",
    "\n",
    "DATA_DIR = \"/model-data/\"\n",
    "DLRM_DIR = os.path.join(DATA_DIR, \"dlrm\")\n",
    "QUERY_TOWER_DIR = os.path.join(DATA_DIR, \"query_tower\")\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "OUTPUT_RETRIEVAL_DATA_DIR = os.path.join(OUTPUT_DATA_DIR, \"retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768637c-0a4d-404b-8b58-7182fef0ab0e",
   "metadata": {},
   "source": [
    "## Define Triton Ensemble\n",
    "In order to run our Recsys in Triton, we need to assemble the pieces that will run together as an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd44a-5a7f-48d9-a363-2094bb24cf90",
   "metadata": {},
   "source": [
    "### Setup Triton Model Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking and retrieval model in the Triton Model Repo. We need to move/copy our trained models from the `DATA_DIR` to the `TRITON_MODEL_REPO` so it can be consumed by Triton on startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_model_path = os.path.join(TRITON_MODEL_REPO, \"1-user-embeddings/1/model.savedmodel/\")\n",
    "ranking_model_path = os.path.join(TRITON_MODEL_REPO, \"5-ranking/1/model.savedmodel/\")\n",
    "\n",
    "# Copy over pretrined Query Tower Model to our Triton Model Repository\n",
    "if not os.path.isdir(retrieval_model_path):\n",
    "    shutil.copytree(QUERY_TOWER_DIR, retrieval_model_path)\n",
    "\n",
    "# Copy over pretrined DLRMfor ranking to our Triton Model Repository\n",
    "if not os.path.isdir(ranking_model_path):\n",
    "    shutil.copytree(DLRM_DIR, ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f38180-3280-4ba6-9f8e-2586b93b9be8",
   "metadata": {},
   "source": [
    "### Explore Triton Model Repo\n",
    "Below we will take a look at the multi-stage ensemble that functions as a DAG of operations within Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99860891-c930-4d34-949a-46187ea77f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/\n",
      "├─0-query-user-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─1-user-embeddings/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2-redis-vss-candidates/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─3-query-item-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─4-unroll-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─5-ranking/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ └─input_schema.json\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─6-softmax-sampling/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "└─ensemble-model/\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "sd.seedir(\n",
    "    TRITON_MODEL_REPO,\n",
    "    style='lines',\n",
    "    itemlimit=10,\n",
    "    depthlimit=5,\n",
    "    exclude_folders=['.ipynb_checkpoints', '__pycache__'],\n",
    "    sort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86781b3-3d1d-4113-bb52-2a86a16037d1",
   "metadata": {},
   "source": [
    "The subfolders (**starting with 0-6**) in the model repo above represent distinct stages in the RecSys ensemble.\n",
    "\n",
    "- `0-query-user-features/` - fetch user features from Redis.\n",
    "- `1-user-embeddings/` - generate user embeddings from the Query Tower (Tensorflow) model.\n",
    "- `2-redis-vss-candidates/` - perform VSS to find KNN items using RediSearch.\n",
    "- `3-query-item-features/` - fetch item features from Redis.\n",
    "- `4-unroll-features/` - combine and unroll user and item features.\n",
    "- `5-ranking/` - rank the top User/Item pairs with the DLRM (Tensorflow) model.\n",
    "- `6-softmax-sampling/` - sort all inputs in descending order, introduce some randomization via softmax sampling, and return top-k ordered items.\n",
    "- `ensemble-model/`\n",
    "\n",
    "The `ensemble-model` contains the orchestration of all of the individual steps. To learn more about general Triton model repo structure [check this out](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7962cc-f26d-4a4a-b5a3-d214e0f37456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Starting Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07c620-7d6c-4275-87fe-e5b94335bdb9",
   "metadata": {},
   "source": [
    "Now we can deploy all the models as an ensemble model to Triton Inference Serve [TIS](https://github.com/triton-inference-server). After we export the ensemble, we are ready to start the TIS. You can start triton server by using the following command on your terminal:\n",
    "\n",
    "```bash\n",
    "tritonserver --model-repository=/workdir/models --backend-config=tensorflow,version=2\n",
    "```\n",
    "\n",
    "*For the `--model-repository` argument, specify the path to the Triton Model Repo stored in the var `TRITON_MODEL_REPO`.* This command will launch the server and load all the models to the server. Once all the models are loaded successfully, you should see `READY` status printed out in the terminal for each loaded model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0794b1-b9e0-4508-bf6e-cc823ac5c693",
   "metadata": {},
   "source": [
    "Once our models are successfully loaded to the TIS, we can now easily send a request to TIS and get a response for our query with our simple Python `client`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9efbde-4dac-42f1-9ace-096f75bac2b5",
   "metadata": {},
   "source": [
    "Let's send a request to TIS for a given `user_id_raw` value. If you make multiple requests in a row for same user, you should see slightly different results based on the randomness introduced via softmax sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d08a8975-9c32-467b-99ec-df66319f854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding recommendations for User 23\n",
      "Recommended Product Ids in 0.9757037162780762 seconds\n",
      "[[ 166]\n",
      " [ 128]\n",
      " [1194]\n",
      " [  94]\n",
      " [ 180]\n",
      " [  79]\n",
      " [  86]\n",
      " [ 171]\n",
      " [ 107]\n",
      " [ 241]\n",
      " [ 551]\n",
      " [ 454]\n",
      " [ 113]\n",
      " [  46]\n",
      " [ 487]\n",
      " [ 370]]\n"
     ]
    }
   ],
   "source": [
    "!python client.py --user 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87bc9f6c-4f24-47d5-921a-6dc1fa903eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Successfully read data for 1 stream/streams with 1 step/steps.\n",
      "*** Measurement Settings ***\n",
      "  Batch size: 1\n",
      "  Service Kind: Triton\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 2\n",
      "  Client: \n",
      "    Request count: 570\n",
      "    Throughput: 31.6623 infer/sec\n",
      "    Avg latency: 63063 usec (standard deviation 30723 usec)\n",
      "    p50 latency: 48007 usec\n",
      "    p90 latency: 121864 usec\n",
      "    p95 latency: 124536 usec\n",
      "    p99 latency: 131981 usec\n",
      "    Avg HTTP time: 63054 usec (send/recv 62 usec + response wait 62992 usec)\n",
      "  Server: \n",
      "    Inference count: 2576\n",
      "    Execution count: 2576\n",
      "    Successful request count: 2576\n",
      "    Avg request latency: 112532 usec (overhead 29 usec + queue 67855 usec + compute 44648 usec)\n",
      "\n",
      "  Composing models: \n",
      "  0-query-user-features, version: \n",
      "      Inference count: 2578\n",
      "      Execution count: 2578\n",
      "      Successful request count: 2578\n",
      "      Avg request latency: 7938 usec (overhead 32 usec + queue 127 usec + compute input 26 usec + compute infer 7647 usec + compute output 105 usec)\n",
      "\n",
      "  1-user-embeddings, version: \n",
      "      Inference count: 2578\n",
      "      Execution count: 2578\n",
      "      Successful request count: 2578\n",
      "      Avg request latency: 1236 usec (overhead 21 usec + queue 24 usec + compute input 48 usec + compute infer 1134 usec + compute output 8 usec)\n",
      "\n",
      "  2-redis-vss-candidates, version: \n",
      "      Inference count: 2578\n",
      "      Execution count: 2578\n",
      "      Successful request count: 2578\n",
      "      Avg request latency: 1888 usec (overhead 8 usec + queue 18 usec + compute input 13 usec + compute infer 1818 usec + compute output 31 usec)\n",
      "\n",
      "  3-query-item-features, version: \n",
      "      Inference count: 2576\n",
      "      Execution count: 2576\n",
      "      Successful request count: 2576\n",
      "      Avg request latency: 98456 usec (overhead 25 usec + queue 67615 usec + compute input 15 usec + compute infer 30696 usec + compute output 105 usec)\n",
      "\n",
      "  4-unroll-features, version: \n",
      "      Inference count: 2576\n",
      "      Execution count: 2576\n",
      "      Successful request count: 2576\n",
      "      Avg request latency: 1077 usec (overhead 51 usec + queue 26 usec + compute input 67 usec + compute infer 791 usec + compute output 141 usec)\n",
      "\n",
      "  5-ranking, version: \n",
      "      Inference count: 2576\n",
      "      Execution count: 2576\n",
      "      Successful request count: 2576\n",
      "      Avg request latency: 1615 usec (overhead 23 usec + queue 26 usec + compute input 84 usec + compute infer 1472 usec + compute output 9 usec)\n",
      "\n",
      "  6-softmax-sampling, version: \n",
      "      Inference count: 2576\n",
      "      Execution count: 2576\n",
      "      Successful request count: 2576\n",
      "      Avg request latency: 462 usec (overhead 9 usec + queue 19 usec + compute input 16 usec + compute infer 343 usec + compute output 74 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 2, throughput: 31.6623 infer/sec, latency 63063 usec\n"
     ]
    }
   ],
   "source": [
    "!perf_analyzer -m ensemble-model -u localhost:8000 --input-data=sample.json --shape=user_id_raw:1,1 -t 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db696892-936b-4021-8549-8b89b0196f1f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "That's it! You finished deploying an online multi-stage Recommender Systems on Triton Inference Server with Redis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow-inference:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
