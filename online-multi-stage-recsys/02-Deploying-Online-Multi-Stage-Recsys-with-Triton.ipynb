{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03166488-1651-4025-84ed-4e9e5db34933",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_merlin_02-deploying-multi-stage-recsys-with-merlin-systems/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Deploying Online Multi-Stage RecSys with Triton Inference Server\n",
    "\n",
    "At this point, we expect that you have already executed the first notebook `01-Building-Online-Multi-Stage-Recsys-Components.ipynb` and exported all the required files and models. \n",
    "\n",
    "We are going to generate recommended items for a given user query (user_id) by following the steps described in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d75184-cd24-4fe3-90f4-d76028626576",
   "metadata": {},
   "source": [
    "![img](./img/OnlineMultiStageRecsys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dadb5-6eec-4a1b-99f9-929523f5cc07",
   "metadata": {},
   "source": [
    "We will serve the multi-stage recommender on [Triton Inference Server](https://github.com/triton-inference-server/server)(TIS) easily and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538677a3-acc6-48f6-acb6-d5bb5fe2e2d2",
   "metadata": {},
   "source": [
    "### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db1b5f1-c8fa-4e03-8744-1197873c5bee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-19 22:38:34.948298: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.EnumValueDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _DATATYPE = _descriptor.EnumDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _TENSORPROTO = _descriptor.Descriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _descriptor.FieldDescriptor(\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "  _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import feast\n",
    "import shutil\n",
    "\n",
    "from nvtabular import ColumnSchema, Schema\n",
    "from merlin.systems.dag.ensemble import Ensemble\n",
    "from merlin.systems.dag.ops.session_filter import FilterCandidates\n",
    "from merlin.systems.dag.ops.softmax_sampling import SoftmaxSampling\n",
    "from merlin.systems.dag.ops.tensorflow import PredictTensorflow\n",
    "from merlin.systems.dag.ops.unroll_features import UnrollFeatures\n",
    "from merlin.systems.triton.utils import send_triton_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b38d23b-dcdd-4e6c-998d-5ed9a5d1c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path for data\n",
    "BASE_DIR = os.environ['PWD']\n",
    "FEATURE_STORE_DIR = os.path.join(BASE_DIR, \"feature_repo/\")\n",
    "TRITON_MODEL_REPO = os.path.join(BASE_DIR, \"models/\")\n",
    "\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "DLRM_DIR = os.path.join(DATA_DIR, \"dlrm\")\n",
    "QUERY_TOWER_DIR = os.path.join(DATA_DIR, \"query_tower\")\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "OUTPUT_RETRIEVAL_DATA_DIR = os.path.join(OUTPUT_DATA_DIR, \"retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768637c-0a4d-404b-8b58-7182fef0ab0e",
   "metadata": {},
   "source": [
    "## Define Triton Ensemble\n",
    "In order to run our Recsys in Triton, we need to assemble the pieces that will run together as an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908dd44a-5a7f-48d9-a363-2094bb24cf90",
   "metadata": {},
   "source": [
    "### Setup Triton Model Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa037c0-7dad-427c-98bb-3da413e8fd14",
   "metadata": {},
   "source": [
    "Define paths for ranking and retrieval model in the Triton Model Repo. We need to move/copy our trained models from the `DATA_DIR` to the `TRITON_MODEL_REPO` so it can be consumed by Triton on startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ba59b5-08c3-44b5-86f2-e63dec6893af",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_model_path = os.path.join(TRITON_MODEL_REPO, \"1-user-embeddings/1/model.savedmodel/\")\n",
    "ranking_model_path = os.path.join(TRITON_MODEL_REPO, \"5-ranking/1/model.savedmodel/\")\n",
    "\n",
    "# Copy over pretrined Query Tower Model to our Triton Model Repository\n",
    "if not os.path.isdir(retrieval_model_path):\n",
    "    shutil.copytree(QUERY_TOWER_DIR, retrieval_model_path)\n",
    "\n",
    "# Copy over pretrined DLRMfor ranking to our Triton Model Repository\n",
    "if not os.path.isdir(ranking_model_path):\n",
    "    shutil.copytree(DLRM_DIR, ranking_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f38180-3280-4ba6-9f8e-2586b93b9be8",
   "metadata": {},
   "source": [
    "### Explore Triton Model Repo\n",
    "Below we will take a look at the multi-stage ensemble that functions as a DAG of operations within Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99860891-c930-4d34-949a-46187ea77f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/\n",
      "├─0-query-user-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─1-user-embeddings/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─assets/\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─2-redis-vss-candidates/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─3-query-item-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─4-unroll-features/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "├─5-ranking/\n",
      "│ ├─1/\n",
      "│ │ └─model.savedmodel/\n",
      "│ │   ├─.merlin/\n",
      "│ │   │ └─input_schema.json\n",
      "│ │   ├─assets/\n",
      "│ │   ├─keras_metadata.pb\n",
      "│ │   ├─saved_model.pb\n",
      "│ │   └─variables/\n",
      "│ │     ├─variables.data-00000-of-00001\n",
      "│ │     └─variables.index\n",
      "│ └─config.pbtxt\n",
      "├─6-softmax-sampling/\n",
      "│ ├─1/\n",
      "│ │ └─model.py\n",
      "│ └─config.pbtxt\n",
      "└─ensemble-model/\n",
      "  ├─1/\n",
      "  └─config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "import seedir as sd\n",
    "\n",
    "sd.seedir(\n",
    "    TRITON_MODEL_REPO,\n",
    "    style='lines',\n",
    "    itemlimit=10,\n",
    "    depthlimit=5,\n",
    "    exclude_folders=['.ipynb_checkpoints', '__pycache__'],\n",
    "    sort=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86781b3-3d1d-4113-bb52-2a86a16037d1",
   "metadata": {},
   "source": [
    "The subfolders (**starting with 0-6**) in the model repo above represent distinct stages in the RecSys ensemble.\n",
    "\n",
    "- `0-query-user-features/` - fetch user features from Redis.\n",
    "- `1-user-embeddings/` - generate user embeddings from the Query Tower (Tensorflow) model.\n",
    "- `2-redis-vss-candidates/` - perform VSS to find KNN items using RediSearch.\n",
    "- `3-query-item-features/` - fetch item features from Redis.\n",
    "- `4-unroll-features/` - combine and unroll user and item features.\n",
    "- `5-ranking/` - rank the top User/Item pairs with the DLRM (Tensorflow) model.\n",
    "- `6-softmax-sampling/` - sort all inputs in descending order, introduce some randomization via softmax sampling, and return top-k ordered items.\n",
    "- `ensemble-model/`\n",
    "\n",
    "The `ensemble-model` contains the orchestration of all of the individual steps. To learn more about general Triton model repo structure [check this out](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7962cc-f26d-4a4a-b5a3-d214e0f37456",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Starting Triton Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c07c620-7d6c-4275-87fe-e5b94335bdb9",
   "metadata": {},
   "source": [
    "Now we can deploy all the models as an ensemble model to Triton Inference Serve [TIS](https://github.com/triton-inference-server). After we export the ensemble, we are ready to start the TIS. You can start triton server by using the following command on your terminal:\n",
    "\n",
    "```bash\n",
    "tritonserver --model-repository=/workdir/models --backend-config=tensorflow,version=2\n",
    "```\n",
    "\n",
    "*For the `--model-repository` argument, specify the path to the Triton Model Repo stored in the var `TRITON_MODEL_REPO`.* This command will launch the server and load all the models to the server. Once all the models are loaded successfully, you should see `READY` status printed out in the terminal for each loaded model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a798f-6abf-4cbb-87f8-f60a6e757092",
   "metadata": {},
   "source": [
    "### Retrieving Recommendations from Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0794b1-b9e0-4508-bf6e-cc823ac5c693",
   "metadata": {},
   "source": [
    "Once our models are successfully loaded to the TIS, we can now easily send a request to TIS and get a response for our query with our simple Python `client`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9efbde-4dac-42f1-9ace-096f75bac2b5",
   "metadata": {},
   "source": [
    "Let's send a request to TIS for a given `user_id_raw` value. If you make multiple requests in a row for same user, you should see slightly different results based on the randomness introduced via softmax sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d08a8975-9c32-467b-99ec-df66319f854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding recommendations for User 23\n",
      "Recommended Product Ids in 0.036287784576416016 seconds\n",
      "[[507]\n",
      " [375]\n",
      " [108]\n",
      " [339]\n",
      " [395]\n",
      " [281]\n",
      " [669]\n",
      " [397]\n",
      " [ 79]\n",
      " [570]\n",
      " [249]\n",
      " [173]\n",
      " [124]\n",
      " [236]\n",
      " [314]\n",
      " [ 48]]\n"
     ]
    }
   ],
   "source": [
    "!python client.py --user 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db696892-936b-4021-8549-8b89b0196f1f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4605dbe-5f97-4b31-8ee4-ce7c1cb69d97",
   "metadata": {},
   "source": [
    "That's it! You finished deploying an online multi-stage Recommender Systems on Triton Inference Server with Redis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow-inference:latest"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
