{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906fc19a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Offline Batch Recommender System\n",
    "\n",
    "In this notebook, we will build a simple offline batch recsys that writes results to Redis for later access. The architecture diagram below shows how this system comes together.\n",
    "\n",
    "![](./img/OfflineBatchRecsys.png)\n",
    "\n",
    "## Candidate Retrieval Model\n",
    "\n",
    "Now about the model itself... many recommender systems have a *two-stage pipeline*:\n",
    "1) A fast **candidate retrieval** model quickly truncates the large item catalog to a relevant set of hundreds (or thousands) of options\n",
    "2) A finely-tuned **ranking model** (i.e. more powerful) ranks the most likely items that are going to interacted with.\n",
    "\n",
    "In this notebook, we will build a simple **Two-Tower** candidate retrieval model with Tensorflow and Merlin/NVTabular helper utilities that can score millions of items for a given user. The Two-Tower model is a neural network architecture with two MLP towers where both user and item features are fed to generate user and item embeddings in the output.\n",
    "\n",
    "Though we skip the ranking model step for now, you will pick that up in the [Multi-Stage Recommender System]() example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf37d1b-6aa8-4df8-9ac2-db968b595078",
   "metadata": {},
   "source": [
    "*This notebook was created using the latest stable [merlin-tensorflow](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow/tags) container and was heavily based on the work done by the NVIDIA Merlin team [here](https://github.com/NVIDIA-Merlin/models/blob/main/examples/05-Retrieval-Model.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401daa6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About the Dataset\n",
    "\n",
    "In this notebook, we use a synthetic dataset that are mimicking the [Ali-CCP: Alibaba Click and Conversion Prediction](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408#1) dataset. The synthetic nature allows us to tune it to our exact needs for demonstration/learning purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac1fea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa8daa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import nvtabular as nvt\n",
    "import merlin.models.tf as mm\n",
    "import tensorflow as tf\n",
    "\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.datasets.synthetic import generate_data\n",
    "from merlin.datasets.ecommerce import transform_aliccp\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac0eef4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate Synthetic Ali-CCP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870acd78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_aliccp_data(num_rows: int, train_size: float, valid_size: float):\n",
    "    train, valid = generate_data(\"aliccp-raw\", num_rows, set_sizes=(train_size, calid_size))\n",
    "    train = train.to_ddf().compute()\n",
    "    valid = valid.to_ddf().compute()\n",
    "    return train, valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd757a3-2f93-495f-b2c5-5098c997d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "\n",
    "NUM_ROWS = 1000000\n",
    "TRAIN_SIZE = 0.7\n",
    "VALID_SIZE = 0.3\n",
    "\n",
    "train, valid = generate_aliccp_data(NUM_ROWS, TRAIN_SIZE, VALID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0454c8-8eec-47cc-82e8-b45637b76d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate datasets to only \"positive\" click interactions between User/Item pairs\n",
    "train = train.loc[train['click']==1].reset_index(drop=True)\n",
    "valid = valid.loc[valid['click']==1].reset_index(drop=True)\n",
    "\n",
    "# Drop the \"target\" interaction fields -- no longer need them\n",
    "train = train.drop(['click', 'conversion'], axis=1)\n",
    "valid = valid.drop(['click', 'conversion'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1787e97-e7c2-4f2b-a728-6329b7d6a8c2",
   "metadata": {},
   "source": [
    "**Note:** To be able to learn from this implicit feedback, we use the naive assumption that the interacted items are **more relevant** for the user than the non-interacted ones.\n",
    "\n",
    "This is an assumption for simplification purposes so we can use a negative sampling technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd9c08-42c6-4aaa-9de5-494e0d5e8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Merlin Dataset wrapper to create dataset objects\n",
    "\n",
    "train = Dataset(train)\n",
    "valid = Dataset(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23232bf-7822-498f-a036-23e44ef48e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defin output path for data\n",
    "\n",
    "DATA_DIR = os.environ['PWD'] +\"data/\"\n",
    "OUTPUT_DATA_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "CATEGORY_TEMP_DIR = os.path.join(DATA_DIR, \"categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078593b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define Feature Transformation Pipeline\n",
    "\n",
    "user_id = [\"user_id\"] >> Categorify(out_path=CATEGORY_TEMP_DIR) >> TagAsUserID()\n",
    "item_id = [\"item_id\"] >> Categorify(out_path=CATEGORY_TEMP_DIR) >> TagAsItemID()\n",
    "\n",
    "item_features = [\"item_category\", \"item_shop\", \"item_brand\"] >> Categorify(out_path=CATEGORY_TEMP_DIR) >> TagAsItemFeatures()\n",
    "\n",
    "user_features = (\n",
    "    [\n",
    "        \"user_shops\",\n",
    "        \"user_profile\",\n",
    "        \"user_group\",\n",
    "        \"user_gender\",\n",
    "        \"user_age\",\n",
    "        \"user_consumption_2\",\n",
    "        \"user_is_occupied\",\n",
    "        \"user_geography\",\n",
    "        \"user_intentions\",\n",
    "        \"user_brands\",\n",
    "        \"user_categories\",\n",
    "    ]\n",
    "    >> Categorify(out_path=CATEGORY_TEMP_DIR)\n",
    "    >> TagAsUserFeatures()\n",
    ")\n",
    "\n",
    "outputs = user_id + item_id + item_features + user_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa99f0b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With `transform_aliccp` function, we can execute fit() and transform() on the raw dataset applying the operators defined in the NVTabular workflow pipeline above. The processed parquet files are saved to output_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab4edc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Transform data and create files\n",
    "transform_aliccp((train, valid), OUTPUT_DATA_DIR, nvt_workflow=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7be4a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building a Two-Tower Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e4a6b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will use Two-Tower Model to infer a subset of relevant items from large item corpus for a given user. \n",
    "\n",
    "A Two-Tower Model consists of item (candidate) and user (query) encoder towers. With two towers, the model can learn representations (embeddings) for queries and candidates separately. \n",
    "\n",
    "> NEED TO FIND IMG\n",
    "<img src=\"./images/TwoTower.png\"  width=\"30%\">\n",
    "\n",
    "Image Adapted from: [Off-policy Learning in Two-stage Recommender Systems](https://dl.acm.org/doi/abs/10.1145/3366423.3380130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f26a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0c257",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load from file\n",
    "train = Dataset(os.path.join(output_path, \"train\", \"*.parquet\"))\n",
    "valid = Dataset(os.path.join(output_path, \"valid\", \"*.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb2dbc-9bbd-46ad-aede-44c3ffbaac9d",
   "metadata": {},
   "source": [
    "Use the `schema` object to define our model. Select features with user and item tags, and be sure to exclude target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb4048",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Schema will consist of the User ID, Item ID, User Features, and Item Features (as defined above)\n",
    "schema = train.schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER])\n",
    "\n",
    "# Set the schema for our datasets\n",
    "train.schema = schema\n",
    "valid.schema = schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3469c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inspect the column names in the schmea here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b18bcf3-65c8-418a-aa43-55abb73a4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095bd9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, we shouldn't have any label/target data yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf449f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label_names = schema.select_by_tag(Tags.TARGET).column_names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd210772-2930-433c-83c3-a1e2dad7d0f6",
   "metadata": {},
   "source": [
    "### About Negative Sampling\n",
    "\n",
    "Many datasets for recommender systems contain implicit feedback with logs of user interactions like clicks, add-to-cart, purchases, music listening events, rather than explicit ratings that reflects user preferences over items. \n",
    "\n",
    "\n",
    "In Merlin Models -- NVIDIA provides some scalable negative sampling algorithms for this Item Retrieval task. In this example, we use the `in-batch` sampling algorithm which uses the items interacted by other users as negatives within the same mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ddb84-fd90-4547-ae58-c8f678d3dcd7",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The **Two-Tower** model consists of a **User tower** (where all user features are fed) and an **Item tower** (where all item features are fed).\n",
    "\n",
    "The User tower generates an embedding for the User. Then it computes the positive interaction \"score\" (likelihood of interaction event) using the dot-product between the User embedding and the Item embedding, in addition to sampled \"negative\" Items within a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30446249-70e8-46c2-98eb-40a5e358d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_tower(tower_dim: int, encoder_dim: int, optimizer: str, k: int, tags) -> mm.TwoTowerModelV2:\n",
    "    # User/Query Tower\n",
    "    user_schema = schema.select_by_tag(tags.USER)\n",
    "    # create user (query) tower input block\n",
    "    user_inputs = mm.InputBlockV2(user_schema)\n",
    "    # create user (query) encoder block\n",
    "    query = mm.Encoder(\n",
    "        user_inputs,\n",
    "        mm.MLPBlock([encoder_dim, tower_dim], no_activation_last_layer=True)\n",
    "    )\n",
    "\n",
    "    # Item/Candidate Tower\n",
    "    item_schema = schema.select_by_tag(tags.ITEM)\n",
    "    # create item (candidate) tower input block\n",
    "    item_inputs = mm.InputBlockV2(item_schema)\n",
    "    # create item (candidate) encoder block\n",
    "    candidate = mm.Encoder(\n",
    "        item_inputs,\n",
    "        mm.MLPBlock([encoder_dim, tower_dim], no_activation_last_layer=True)\n",
    "    )\n",
    "    \n",
    "    # Build Model Class\n",
    "    model = mm.TwoTowerModelV2(query, candidate)\n",
    "    model.compile(optimizer=optimizer, run_eagerly=False, metrics=[mm.RecallAt(k), mm.NDCGAt(k)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25407870-5e59-4562-8c39-2869a13ac8bd",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- `no_activation_last_layer:` when set True, no activation is used for top hidden layer. Learn more [here](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b9f4e78a8830fe5afcf2f0452862fb3c0d6584ea.pdf).\n",
    "- In the `TwoTowerModelV2` function we did not set `negative_samplers` arg. By default, it uses contrastive learning and `in-batch` negative sampling strategy.\n",
    "- Two metrics are used to judge the quality of the recommendations: **Normalized Discounted Cumulative Gain (NDCG@K)** and **Recall@K**.\n",
    "    - NDCG@K accounts for rank of the relevant item in the recommendation list and is a more fine-grained metric than HR, which only verifies whether the relevant item is among the top-k items.\n",
    "    - Recall (Also known as HitRate@K) when there is only one relevant item in the recommendation list. Recall just verifies whether the relevant item is among the top-k items.\n",
    "- When we set `validation_data=valid` in the `model.fit()`, we compute evaluation metrics on validation set using the negative sampling strategy used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280bb4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = create_two_tower(\n",
    "    tower_dim=64,\n",
    "    encoder_dim=128,\n",
    "    optimizer=\"adam\",\n",
    "    k=10,\n",
    "    tags=Tags\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "model.fit(train, validation_data=valid, batch_size=4096, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c10c2-ae46-45bf-9623-6214c7570ac8",
   "metadata": {},
   "source": [
    "### Evaluate the model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155af447-97c4-4875-97ad-84e678fd7b40",
   "metadata": {},
   "source": [
    "The validation metric values during training are calculated given the positive and negative scores in each batch, and then averaged over batches per epoch. **That means validation metrics are not computed using the entire item catalog.**\n",
    "\n",
    "To determine the exact accuracy, we need to compute the similarity score between a given query and all possible candidates. Below, by using the `topk_model` we can evaluate the trained retrieval model using the entire item catalog (brute force)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a15e73-4835-4720-bb64-1d745b3b3f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create candidate/item features for evaluation\n",
    "candidate_features = unique_rows_by_features(train, Tags.ITEM, Tags.ITEM_ID)\n",
    "candidate_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bbf45-2afa-4b1a-ae96-a3a4e525de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to a top_k_encoder\n",
    "topk_model = model.to_top_k_encoder(candidate_features, k=20, batch_size=128)\n",
    "\n",
    "# we can set `metrics` param in the `compile(), if we want\n",
    "topk_model.compile(run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44193468-c5ab-43ba-967c-605f646695b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42eaa0-996e-43e3-a951-fc013e2d0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for validation data\n",
    "eval_loader = mm.Loader(valid, batch_size=1024).map(mm.ToTarget(schema, \"item_id\"))\n",
    "\n",
    "# Evaluation\n",
    "metrics = topk_model.evaluate(eval_loader, return_dict=True)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49037a0-a1c2-4a6f-a89e-e67e0e696445",
   "metadata": {},
   "source": [
    "### Generate top-K recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a1acf-5baf-4f38-8ce4-2c05c6e685cb",
   "metadata": {},
   "source": [
    "We trained a model, now we can generate recommendations offline using `to_top_k_encoder` method. The `to_top_k_encoder()` uses the pre-trained candidate and query encoders to initialize a top-k encoder model, called as `topk_model` in this example. Practically, this method applies the candidate_encoder on the provided candidate_features dataset to set the top-k index of the `topk_model`. Therefore, topk_model object is the one responsible of generating the top-k predictions.\n",
    "\n",
    "Let's generate top-K (k=20 in our example) recommendations for a given batch of 8 samples. The `to_top_k_encoder()` method uses the candidate (item) features dataset as the identifiers, i.e., we extract the`  candidate_id` arg using `Tags.ITEM_ID` tag by default and set it as `index` when calculating the candidate embeddings. The forward method of `topk_model` takes as the query features as input, and computes the dot product scores between the given query embeddings and all the candidates of the top-k index. Then, it returns the top-k (k=20) item ids with the highest scores. Note that instead of calculating the candidate (item) tower embeddings for each user query, we compute the output of the item tower once and store it in the `TopKEncoder` class  to use for the Top-k index. This is computationally more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3c032-0084-46c8-b61c-eb7b8595d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk_recs(user_count: int, valid)\n",
    "    eval_loader = mm.Loader(valid, batch_size=user_count, shuffle=False)\n",
    "    for batch in loader\n",
    "    batch = next(iter(eval_loader))\n",
    "    print(batch[0]['user_id'])\n",
    "    return topk_model(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b8048-1a90-4311-8d19-be6e8715fc7f",
   "metadata": {},
   "source": [
    "The recommended top 20 item ids are returned below for each of the 8 selected users (from the validation set). The output of the method is a named tuple `TopKPrediction`, where the first element is the dot product scores and the second element is the encoded item ids (not the original ids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e720af4-70cd-40c8-857b-75e127e7d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_topk_recs(8, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea442dd4-e570-4500-80ae-7996f9acaed5",
   "metadata": {},
   "source": [
    "## Writing Recommendations to the Inference Store\n",
    "\n",
    "Redis is used (low latency k-v store) to persist recommendations for each User.\n",
    "\n",
    "### Simple Redis Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97aa11-1430-4f8c-866a-6c7c1d76e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "redis_conn = redis.Redis(\n",
    "    host=\"redis-inference-store\",\n",
    "    port=6379\n",
    ")\n",
    "\n",
    "# Simple HASH set\n",
    "redis_conn.hset(\"test\", mapping={\"id\": 1, \"foo\": \"bar\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32734d-17ff-4ee7-ac5d-9840bf1c9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Hash get\n",
    "redis_conn.hgetall(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47c2fe-a7a7-4ff8-94db-ebe2401e79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "redis_conn.delete(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd929c-ec80-49d8-ba44-7a7a81ce9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import redis.asyncio as redis\n",
    "\n",
    "from redis.commands.json.path import Path\n",
    "\n",
    "\n",
    "def generate_topk_recs(topk_model, K: int, batch_size: int):\n",
    "    loader = mm.Loader(valid, batch_size=batch_size, shuffle=False)\n",
    "    for batch in loader\n",
    "        results = topk_model(batch[0])\n",
    "        # TODO - need to inspect this result to figure out how to grab the top K\n",
    "        # TODO figure the generator part out\n",
    "        for user_id, res in results:\n",
    "            yield user_id, res\n",
    "\n",
    "async def store_recommendations(n: int, redis_conn: redis.Redis):\n",
    "    \"\"\"\n",
    "    Store recommendations generated for each User.\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(n)\n",
    "    async def store(user_id: str, recs: list):\n",
    "        \"\"\"\n",
    "        Store and individual User's latest recommendations in Redis.\n",
    "        \"\"\"\n",
    "        async with semaphore:\n",
    "            entry = {\n",
    "                \"id\": user_id,\n",
    "                \"recommendations\": recommmendations\n",
    "            }\n",
    "            # Set the JSON object in Redis\n",
    "            await redis_conn.json().set(f\"USER:{user_id}\", Path.root_path(), entry)\n",
    "    \n",
    "    # create generator\n",
    "    topk_recs_per_user = generate_topk_recs(topk_model, K, batch_size)\n",
    "    # gather with \"concurrency\"\n",
    "    await asyncio.gather(*[store(user_id, recs) for user_id, recs in topk_recs_per_user])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd57904-4419-4b77-9ece-eb323c0a3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_conn = redis.Redis(\n",
    "    host=\"redis-inference-store\",\n",
    "    port=6379\n",
    ")\n",
    "\n",
    "# Run the process\n",
    "asyncio.run(store_recommendations(100, redis_conn, topk_model, K=10, batch_size=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74321a66-051d-410a-a9ae-8fa7d81b5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "redis_conn.dbsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19062ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exporting Retrieval Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8cb35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So far we have trained and evaluated our Retrieval model. Now, the next step is to deploy our model and generate top-K recommendations given a user (query). We can efficiently serve our model by indexing the trained item embeddings into an **Approximate Nearest Neighbors (ANN)** engine. Basically, for a given user query vector, that is generated passing the user features into user tower of retrieval model, we do an ANN search query to find the ids of nearby item vectors, and at serve time, we score user embeddings over all indexed top-K item embeddings within the ANN engine.\n",
    "\n",
    "In doing so, we need to export\n",
    " \n",
    "- user (query) tower\n",
    "- item and user features\n",
    "- item embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01df11e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Save and Load User (query) tower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc5e2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are able to save the user tower model as a TF model to disk. The user tower model is needed to generate a user embedding vector when a user feature vector <i>x</i> is fed into that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c68eb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query_tower = model.query_encoder\n",
    "query_tower.save(os.path.join(DATA_FOLDER, \"query_tower\"))\n",
    "\n",
    "## we can load back the saved model via the following script.\n",
    "#query_tower_loaded = tf.keras.models.load_model(os.path.join(DATA_FOLDER, 'query_tower'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c031f8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extract and save User features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd88a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With `unique_rows_by_features` utility function we can easily extract both unique user and item features tables as cuDF dataframes. Note that for user features table, we use `USER` and `USER_ID` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f861ac4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_features = (\n",
    "    unique_rows_by_features(train, Tags.USER, Tags.USER_ID).compute().reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fcb3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33758417",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "user_features.to_parquet(os.path.join(DATA_FOLDER, \"user_features.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f6cf6-cc5e-459e-9d87-5fdcd01d8c85",
   "metadata": {},
   "source": [
    "#### Generate Query embeddings for entire user catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31a12c-ca41-4b02-9288-7d6b31d54e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = model.query_embeddings(Dataset(user_features, schema=schema), batch_size=1024, index=Tags.USER_ID)\n",
    "query_embs_df = queries.compute(scheduler=\"synchronous\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b45f67-b3b3-42c3-8d76-b7ac0e391d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b1578",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extract and save Item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae320ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "item_features = (\n",
    "    unique_rows_by_features(train, Tags.ITEM, Tags.ITEM_ID).compute().reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24dcb8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "item_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40102380",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_features.to_parquet(os.path.join(DATA_FOLDER, \"item_features.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f5a3e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extract and save Item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f50d2a-dbf3-4459-af46-1a8194730896",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embs = model.candidate_embeddings(Dataset(item_features, schema=schema), batch_size=1024, index=Tags.ITEM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138678ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "item_embs_df = item_embs.compute(scheduler=\"synchronous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69694b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "item_embs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d152e82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save to disk\n",
    "item_embs_df.to_parquet(os.path.join(DATA_FOLDER, \"item_embeddings.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed43c3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That's it. You have learned how to train and evaluate your Two-Tower retrieval model, and then how to export the required components to be able to deploy this model to generate recommendations. In order to learn more on serving a model to [Triton Inference Server](https://github.com/triton-inference-server/server), please explore the examples in the [Merlin](https://github.com/NVIDIA-Merlin/Merlin) and [Merlin Systems](https://github.com/NVIDIA-Merlin/systems) repos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "merlin": {
   "containers": [
    "nvcr.io/nvidia/merlin/merlin-tensorflow:latest"
   ]
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
